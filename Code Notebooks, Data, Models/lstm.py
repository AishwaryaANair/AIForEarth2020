# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rBjssyUFzn6ffwpKwj8kppHHbXFR8ZO5
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 22, 10

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

!pip freeze > requirements.txt

dfx = pd.read_pickle(
  "FFStar.pkl",
#  parse_dates=[0],
#  index_col= 0,
#  header = None
)

dfx.head()

cols = ['lat','long','max_temp','sun_hour','dew_point','heat_index','cloud_cover','humid','precp','wind_speed','fire_or_not','elevation']
dfx.columns = cols
dfx.head()

df1 = dfx.copy()

dfx = dfx.drop(['fire_or_not','dew_point','sun_hour','heat_index', 'long','lat'], axis=1)

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(dfx.values, i) for i in range(dfx.shape[1])]
vif["features"] = dfx.columns

vif

df = df1.copy()

train_size = int(len(df) * 0.8)
test_size = len(df) - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]

#train_size = int(len(df) * 0.8)
#test_size = len(df) - train_size
train, test = pd.read_pickle('train.pkl'), pd.read_pickle('test.pkl')
print(len(train), len(test))

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)        
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

from sklearn.preprocessing import RobustScaler

f_columns = ['max_temp','cloud_cover','humid','precp','wind_speed','elevation']

f_transformer = RobustScaler()
cnt_transformer = RobustScaler()

f_transformer = f_transformer.fit(train[f_columns].to_numpy())
cnt_transformer = cnt_transformer.fit(train[['lat']])

train.loc[:, f_columns] = f_transformer.transform(train[f_columns].to_numpy())
train['lat'] = cnt_transformer.transform(train[['lat']])

test.loc[:, f_columns] = f_transformer.transform(test[f_columns].to_numpy())
test['lat'] = cnt_transformer.transform(test[['lat']])

train.loc[:, f_columns] = f_transformerlong.transform(train[f_columns].to_numpy())
train['long'] = cnt_transformerlong.transform(train[['long']])

test.loc[:, f_columns] = f_transformerlong.transform(test[f_columns].to_numpy())
test['long'] = cnt_transformerlong.transform(test[['long']])

df.head()

import joblib
joblib.dump(f_transformer, "feature_transformerlat.joblib")

joblib.dump(cnt_transformer, "cnt_transformerlat.joblib")

time_steps = 10

# reshape to [samples, time_steps, n_features]

X_train1, y_train1 = create_dataset(train, train.lat, time_steps)
X_test1, y_test1 = create_dataset(test, test.lat, time_steps)

model = keras.Sequential()
model.add(
  keras.layers.Bidirectional(
    keras.layers.LSTM(
      units=128, 
      input_shape=(X_train1.shape[1], X_train1.shape[2])
    )
  )
)
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.Dense(units=100))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse'])

history = model.fit(
    X_train1, y_train1, 
    epochs=50, 
    batch_size=72, 
    validation_split=0.1,
    shuffle=False
)

scores = model.evaluate(X_test1, y_test1, verbose=0)
print(scores)

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend();

y_pred1 = model.predict(X_test1)

y_train_inv1 = cnt_transformer.inverse_transform(y_train1.reshape(1, -1))
y_test_inv1 = cnt_transformer.inverse_transform(y_test1.reshape(1, -1))
y_pred_inv1 = cnt_transformer.inverse_transform(y_pred1)

print(y_pred_inv1)

from matplotlib import pyplot
pyplot.plot(history.history['loss'])
pyplot.plot(history.history['val_loss'])
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.legend(['train', 'validation'], loc='upper right')
pyplot.show()

import pickle
pickle.dump(y_pred_inv1, open('latpred.pkl', 'wb'))

import pickle
pickle.dump(X_test1, open('X_test_lat.pkl', 'wb'))

print(pickle.load(open('latpred.pkl', 'rb')))

import math
from sklearn.metrics import mean_squared_error
rmse = math.sqrt(mean_squared_error(y_pred1, y_test1))
print('Test RMSE: %.3f' % rmse)

from sklearn.metrics import r2_score
print(r2_score(y_test1, y_pred1))

print(model.summary())

import azureml.core


# Check core SDK version number.
print('SDK version:', azureml.core.VERSION)

from azureml.core import Workspace


ws = Workspace.from_config()
print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\n')

ws.write_config(path="./", file_name="ws_config.json")
# View your configuration file
!cat .azureml/ws_config.json

import os
import h5py
from sklearn import datasets 
 
# save model
# serialize model to JSON
LSTM_MODEL = 'modelLat.json'

# and store the weights in h5
MODEL_WEIGHTS = 'modellat.h5'


model_json = model.to_json()
with open(LSTM_MODEL, "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights(MODEL_WEIGHTS)
print("Model saved")

modellong.save('my_modellong/1')

modellong.save('LONGLstm.h5')

df = dfx.copy()

dfx = df.copy()

dfx

dfx = dfx.drop(['fire_or_not','lat','dew_point'], axis=1)

train_size = int(len(dfx) * 0.8)
test_size = len(dfx) - train_size
trainX, testX = dfx.iloc[0:train_size], dfx.iloc[train_size:len(df)]

joblib.dump(cnt_transformerlong, "cnt_transformerlong.joblib")

joblib.dump(feature_transformerlong, "ft_transformerlong.joblib")

dfx = df.copy()

time_steps = 5

# reshape to [samples, time_steps, n_features]

X_trainlong, y_trainlong = create_dataset(train, train, time_steps)
X_testlong, y_testlong = create_dataset(test, test, time_steps)

modellong = keras.Sequential()
modellong.add(
  keras.layers.Bidirectional(
    keras.layers.LSTM(
      units=128, 
      input_shape=(X_trainlong.shape[1], X_trainlong.shape[2])
    )
  )
)
modellong.add(keras.layers.Dropout(rate=0.2))
modellong.add(keras.layers.Dense(128))
modellong.add(keras.layers.Dense(1, activation='sigmoid'))
modellong.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse'])

history2 = modellong.fit(
    X_trainlong, y_trainlong, 
    epochs=100, 
    batch_size=1000, 
    validation_split=0.1,
    shuffle=False
)

scores = modellong.evaluate(X_testlong, y_testlong, verbose=0)
print(scores)

plt.plot(history2.history['loss'], label='train')
plt.plot(history2.history['val_loss'], label='test')
plt.legend();

y_pred = modellong.predict(X_testlong)

import pickle
pickle.dump(y_pred, open('longpred.pkl', 'wb'))

import pickle
pickle.dump(X_testlong, open('X_testlong.pkl', 'wb'))

y_train_inv = cnt_transformerlong.inverse_transform(y_trainlong.reshape(1, -1))
y_test_inv = cnt_transformerlong.inverse_transform(y_testlong.reshape(1, -1))
y_pred_inv = cnt_transformerlong.inverse_transform(y_pred)

print(y_pred_inv)

import pickle
pickle.dump(y_pred_inv1, open('latpred.pkl', 'wb'))

import math
from sklearn.metrics import mean_squared_error
rmse = math.sqrt(mean_squared_error(y_pred, y_test))
print('Test RMSE: %.3f' % rmse)

import os
import h5py
from sklearn import datasets 
 
# save model
# serialize model to JSON
LSTM_MODEL = 'modelLong.json'

# and store the weights in h5
MODEL_WEIGHTS = 'modellong.h5'


model_json = modellong.to_json()
with open(LSTM_MODEL, "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model1.save_weights(MODEL_WEIGHTS)
print("Model saved")

import plotly.express as px
import geopandas as gpd

geo_df = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))

px.set_mapbox_access_token(open(".mapbox_token").read())
fig = px.scatter_geo(geo_df,
                    lat=geo_df.geometry.y,
                    lon=geo_df.geometry.x,
                    hover_name="name")
fig.show()

model.save('LSTMTest.h5')

df1 = pd.read_csv(
  "FF.csv",
  parse_dates=[0],
  index_col= 0,
  header = None
)

df1.head()

df1 = df1.drop([1,2], axis = 1)

df1.to_csv('dep.csv')

import plotly
import chart_studio.plotly as py
import plotly.graph_objs as go
plotly.offline.init_notebook_mode()

import pandas as pd
lat = pd.DataFrame(y_pred_inv1)
long = pd.DataFrame(y_pred_inv)
coords = pd.concat([lat,long], axis=1)
coords.columns = ['lat','long']

cases = []
colors = ['rgb(239,243,255)','rgb(189,215,231)','rgb(107,174,214)','rgb(33,113,181)']
for i in range(6,10)[::-1]:
    cases.append(go.Scattergeo(
        lon = coords.long,
        lat = coords.lat,
        marker = dict(
            color = 'black',
            opacity = .4,
            line = dict(width = 0)
        ),
    ) )

cases[0]['mode'] = 'markers'

layout = go.Layout(
    title = 'Hey look! It\'s a scatter plot on a map!',
    geo = dict(
        resolution = 110,
        scope = 'world',
        showframe = False,
        showcoastlines = True,
        showland = True,
        landcolor = "rgb(229, 229, 229)",
        countrycolor = "rgb(255, 255, 255)" ,
        coastlinecolor = "rgb(255, 255, 255)",
        projection = dict(
            type = 'mercator'
        ),        
    ),
    legend = dict(
           traceorder = 'reversed'
    )
)


fig = go.Figure(layout=layout, data=cases)
plotly.offline.iplot(fig, validate=False, filename='iantest')

!pip install chart-studio

!pip install geopandas

import plotly.graph_objects as go

import pandas as pd

fig = go.Figure(data=go.Scattergeo(
        lon = coords['long'],
        lat = coords['lat'],
        mode = 'markers',
        marker_color = 'black',
        ))

fig.update_layout(
        title = 'ff',
        geo_scope='asia',
    )
fig.show()